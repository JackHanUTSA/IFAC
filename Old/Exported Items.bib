
@article{liu_enhancing_2024,
	title = {Enhancing the {LLM}-{Based} {Robot} {Manipulation} {Through} {Human}-{Robot} {Collaboration}},
	volume = {9},
	issn = {2377-3766, 2377-3774},
	url = {http://arxiv.org/abs/2406.14097},
	doi = {10.1109/LRA.2024.3415931},
	abstract = {Large Language Models (LLMs) are gaining popularity in the field of robotics. However, LLM-based robots are limited to simple, repetitive motions due to the poor integration between language models, robots, and the environment. This paper proposes a novel approach to enhance the performance of LLM-based autonomous manipulation through Human-Robot Collaboration (HRC). The approach involves using a prompted GPT-4 language model to decompose high-level language commands into sequences of motions that can be executed by the robot. The system also employs a YOLO-based perception algorithm, providing visual cues to the LLM, which aids in planning feasible motions within the specific environment. Additionally, an HRC method is proposed by combining teleoperation and Dynamic Movement Primitives (DMP), allowing the LLM-based robot to learn from human guidance. Real-world experiments have been conducted using the Toyota Human Support Robot for manipulation tasks. The outcomes indicate that tasks requiring complex trajectory planning and reasoning over environments can be efficiently accomplished through the incorporation of human demonstrations.},
	language = {en},
	number = {8},
	urldate = {2025-11-17},
	journal = {IEEE Robotics and Automation Letters},
	author = {Liu, Haokun and Zhu, Yaonan and Kato, Kenji and Tsukahara, Atsushi and Kondo, Izumi and Aoyama, Tadayoshi and Hasegawa, Yasuhisa},
	month = aug,
	year = {2024},
	note = {arXiv:2406.14097 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Robotics},
	pages = {6904--6911},
	annote = {Comment: IEEE Robotics and Automation Letters},
}

@misc{bhat_grounding_2024,
	title = {Grounding {LLMs} {For} {Robot} {Task} {Planning} {Using} {Closed}-loop {State} {Feedback}},
	url = {http://arxiv.org/abs/2402.08546},
	doi = {10.48550/arXiv.2402.08546},
	abstract = {Planning algorithms decompose complex problems into intermediate steps that can be sequentially executed by robots to complete tasks. Recent works have employed Large Language Models (LLMs) for task planning, using natural language to generate robot policies in both simulation and real-world environments. LLMs like GPT-4 have shown promising results in generalizing to unseen tasks, but their applicability is limited due to hallucinations caused by insufficient grounding in the robot environment. The robustness of LLMs in task planning can be enhanced with environmental state information and feedback. In this paper, we introduce a novel approach to task planning that utilizes two separate LLMs for high-level planning and low-level control, improving task-related success rates and goal condition recall. Our algorithm, BrainBody-LLM, draws inspiration from the human neural system, emulating its brain-body architecture by dividing planning across two LLMs in a structured, hierarchical manner. BrainBody-LLM implements a closed-loop feedback mechanism, enabling learning from simulator errors to resolve execution errors in complex settings. We demonstrate the successful application of BrainBody-LLM in the VirtualHome simulation environment, achieving a 29\% improvement in taskoriented success rates over competitive baselines with the GPT-4 backend. Additionally, we evaluate our algorithm on seven complex tasks using a realistic physics simulator and the Franka Research 3 robotic arm, comparing it with various state-of-the-art LLMs. Our results show advancements in the reasoning capabilities of recent LLMs, which enable them to learn from raw simulator/controller errors to correct plans, making them highly effective in robotic task planning. Demo Video: http: //tinyurl.com/2akwhvf2.},
	language = {en},
	urldate = {2025-11-17},
	publisher = {arXiv},
	author = {Bhat, Vineet and Kaypak, Ali Umut and Krishnamurthy, Prashanth and Karri, Ramesh and Khorrami, Farshad},
	month = aug,
	year = {2024},
	note = {arXiv:2402.08546 [cs]},
	keywords = {Computer Science - Robotics},
	annote = {Comment: This work has been submitted to Autonomous Robots},
}

@misc{strobel_llm2swarm_2024,
	title = {{LLM2Swarm}: {Robot} {Swarms} that {Responsively} {Reason}, {Plan}, and {Collaborate} through {LLMs}},
	shorttitle = {{LLM2Swarm}},
	url = {http://arxiv.org/abs/2410.11387},
	doi = {10.48550/arXiv.2410.11387},
	abstract = {Robot swarms are composed of many simple robots that communicate and collaborate to fulfill complex tasks. Robot controllers usually need to be specified by experts on a case-by-case basis via programming code. This process is timeconsuming, prone to errors, and unable to take into account all situations that may be encountered during deployment. On the other hand, recent Large Language Models (LLMs) have demonstrated reasoning and planning capabilities, introduced new ways to interact with and program machines, and incorporate both domain-specific and commonsense knowledge. Hence, we propose to address the aforementioned challenges by integrating LLMs with robot swarms and show the potential in proofs of concept (showcases). For this integration, we explore two approaches. The first approach is ‘indirect integration,’ where LLMs are used to synthesize and validate the robot controllers. This approach may reduce development time and human error before deployment. Moreover, during deployment, it could be used for on-the-fly creation of new robot behaviors. The second approach is ‘direct integration,’ where each robot locally executes a separate LLM instance during deployment for robotrobot collaboration and human-swarm interaction. These local LLM instances enable each robot to reason, plan, and collaborate using natural language, as demonstrated in our showcases where the robots are able to detect a variety of anomalies, without prior information about the nature of these anomalies. To enable further research on our mainly conceptual contribution, we release the software and videos for our LLM2Swarm system: https://github.com/Pold87/LLM2Swarm.},
	language = {en},
	urldate = {2025-11-17},
	publisher = {arXiv},
	author = {Strobel, Volker and Dorigo, Marco and Fritz, Mario},
	month = oct,
	year = {2024},
	note = {arXiv:2410.11387 [cs]},
	keywords = {Computer Science - Robotics},
	annote = {Comment: Accepted at NeurIPS 2024 Workshop on Open-World Agents. Code: https://github.com/Pold87/LLM2Swarm/},
}

@misc{yang_llm-grounder_2023,
	title = {{LLM}-{Grounder}: {Open}-{Vocabulary} {3D} {Visual} {Grounding} with {Large} {Language} {Model} as an {Agent}},
	shorttitle = {{LLM}-{Grounder}},
	url = {http://arxiv.org/abs/2309.12311},
	doi = {10.48550/arXiv.2309.12311},
	abstract = {3D visual grounding is a critical skill for household robots, enabling them to navigate, manipulate objects, and answer questions based on their environment. While existing approaches often rely on extensive labeled data or exhibit limitations in handling complex language queries, we propose LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model (LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to decompose complex natural language queries into semantic constituents and employs a visual grounding tool, such as OpenScene or LERF, to identify objects in a 3D scene. The LLM then evaluates the spatial and commonsense relations among the proposed objects to make a final grounding decision. Our method does not require any labeled training data and can generalize to novel 3D scenes and arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and demonstrate state-of-the-art zero-shot grounding accuracy. Our findings indicate that LLMs significantly improve the grounding capability, especially for complex language queries, making LLM-Grounder an effective approach for 3D vision-language tasks in robotics. Videos and interactive demos can be found on the project website https://chat-with-nerf.github.io/ .},
	language = {en},
	urldate = {2025-11-17},
	publisher = {arXiv},
	author = {Yang, Jianing and Chen, Xuweiyi and Qian, Shengyi and Madaan, Nikhil and Iyengar, Madhavan and Fouhey, David F. and Chai, Joyce},
	month = sep,
	year = {2023},
	note = {arXiv:2309.12311 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Project website: https://chat-with-nerf.github.io/},
}

@article{xu_penetrative_nodate,
	title = {Penetrative {AI}: {Making} {LLMs} {Comprehend} the {Physical} {World}},
	abstract = {Recent developments in Large Language Models (LLMs) have demonstrated their remarkable capabilities across a range of tasks. Questions, however, persist about the nature of LLMs and their potential to integrate common-sense human knowledge when performing tasks involving information about the real physical world. This paper delves into these questions by exploring how LLMs can be extended to interact with and reason about the physical world through IoT sensors and actuators, a concept that we term "Penetrative AI1". The paper explores such an extension at two levels of LLMs’ ability to penetrate into the physical world via the processing of sensory signals. Our preliminary findings indicate that LLMs, with ChatGPT being the representative example in our exploration, have considerable and unique proficiency in employing the embedded world knowledge for interpreting IoT sensor data and reasoning over them about tasks in the physical realm. Not only this opens up new applications for LLMs beyond traditional textbased tasks, but also enables new ways of incorporating human knowledge in cyber-physical systems.},
	language = {en},
	author = {Xu, Huatao and Han, Liying and Yang, Qirui and Li, Mo and Srivastava, Mani},
}

@misc{wang_prompt_2024,
	title = {Prompt a {Robot} to {Walk} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2309.09969},
	doi = {10.48550/arXiv.2309.09969},
	abstract = {Large language models (LLMs) pre-trained on vast internet-scale data have showcased remarkable capabilities across diverse domains. Recently, there has been escalating interest in deploying LLMs for robotics, aiming to harness the power of foundation models in real-world settings. However, this approach faces significant challenges, particularly in grounding these models in the physical world and in generating dynamic robot motions. To address these issues, we introduce a novel paradigm in which we use few-shot prompts collected from the physical environment, enabling the LLM to autoregressively predict low-level control actions for robots without task-specific fine-tuning. We utilize LLMs as a controller, diverging from the conventional approach of employing them primarily as planners. Simulation experiments across various robots and environments validate that our method can effectively prompt a robot to walk. We thus illustrate how LLMs can function as low-level feedback controllers for dynamic motion control, even in high-dimensional robotic systems. The project website and source code can be found at: prompt2walk.github.io.},
	language = {en},
	urldate = {2025-11-17},
	publisher = {arXiv},
	author = {Wang, Yen-Jen and Zhang, Bike and Chen, Jianyu and Sreenath, Koushil},
	month = oct,
	year = {2024},
	note = {arXiv:2309.09969 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control},
	annote = {Comment: Conference on Decision and Control (CDC), 2024},
}

@article{noauthor_robot_2025,
	title = {Robot planning with {LLMs}},
	volume = {7},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-025-01036-4},
	doi = {10.1038/s42256-025-01036-4},
	language = {en},
	number = {4},
	urldate = {2025-11-17},
	journal = {Nature Machine Intelligence},
	month = apr,
	year = {2025},
	pages = {521--521},
}

@article{andrychowicz_hindsight_nodate,
	title = {Hindsight {Experience} {Replay}},
	abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efﬁcient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task. The video presenting our experiments is available at https://goo.gl/SMrQnI.},
	language = {en},
	author = {Andrychowicz, Marcin and Crow, Dwight and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, OpenAI Pieter and Zaremba, Wojciech},
}

@misc{huang_inner_2022,
	title = {Inner {Monologue}: {Embodied} {Reasoning} through {Planning} with {Language} {Models}},
	shorttitle = {Inner {Monologue}},
	url = {http://arxiv.org/abs/2207.05608},
	doi = {10.48550/arXiv.2207.05608},
	abstract = {Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent’s own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.},
	language = {en},
	urldate = {2025-11-28},
	publisher = {arXiv},
	author = {Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and Sermanet, Pierre and Brown, Noah and Jackson, Tomas and Luu, Linda and Levine, Sergey and Hausman, Karol and Ichter, Brian},
	month = jul,
	year = {2022},
	note = {arXiv:2207.05608 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Project website: https://innermonologue.github.io},
}

@misc{makoviychuk_isaac_2021,
	title = {Isaac {Gym}: {High} {Performance} {GPU}-{Based} {Physics} {Simulation} {For} {Robot} {Learning}},
	shorttitle = {Isaac {Gym}},
	url = {http://arxiv.org/abs/2108.10470},
	doi = {10.48550/arXiv.2108.10470},
	abstract = {Isaac Gym offers a high performance learning platform to train policies for wide variety of robotics tasks directly on GPU. Both physics simulation and the neural network policy training reside on GPU and communicate by directly passing data from physics buffers to PyTorch tensors without ever going through any CPU bottlenecks. This leads to blazing fast training times for complex robotics tasks on a single GPU with 2-3 orders of magnitude improvements compared to conventional RL training that uses a CPU based simulator and GPU for neural networks. We host the results and videos at {\textbackslash}url\{https://sites.google.com/view/isaacgym-nvidia\} and isaac gym can be downloaded at {\textbackslash}url\{https://developer.nvidia.com/isaac-gym\}.},
	language = {en},
	urldate = {2025-11-28},
	publisher = {arXiv},
	author = {Makoviychuk, Viktor and Wawrzyniak, Lukasz and Guo, Yunrong and Lu, Michelle and Storey, Kier and Macklin, Miles and Hoeller, David and Rudin, Nikita and Allshire, Arthur and Handa, Ankur and State, Gavriel},
	month = aug,
	year = {2021},
	note = {arXiv:2108.10470 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Machine Learning},
	annote = {Comment: tech report on isaac-gym},
}

@article{huang_language_nodate,
	title = {Language {Models} as {Zero}-{Shot} {Planners}: {Extracting} {Actionable} {Knowledge} for {Embodied} {Agents}},
	abstract = {Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. “make breakfast”), to a chosen set of actionable steps (e.g. “open fridge”). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into midlevel plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models.},
	language = {en},
	author = {Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
}

@misc{huang_language_2022,
	title = {Language {Models} as {Zero}-{Shot} {Planners}: {Extracting} {Actionable} {Knowledge} for {Embodied} {Agents}},
	shorttitle = {Language {Models} as {Zero}-{Shot} {Planners}},
	url = {http://arxiv.org/abs/2201.07207},
	doi = {10.48550/arXiv.2201.07207},
	abstract = {Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. “make breakfast”), to a chosen set of actionable steps (e.g. “open fridge”). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly ﬁnd that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models.},
	language = {en},
	urldate = {2025-11-28},
	publisher = {arXiv},
	author = {Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
	month = mar,
	year = {2022},
	note = {arXiv:2201.07207 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Project website at https://huangwl18.github.io/language-planner},
}

@misc{mirchandani_large_2023,
	title = {Large {Language} {Models} as {General} {Pattern} {Machines}},
	url = {http://arxiv.org/abs/2307.04721},
	doi = {10.48550/arXiv.2307.04721},
	abstract = {We observe that pre-trained large language models (LLMs) are capable of autoregressively completing complex token sequences—from arbitrary ones procedurally generated by probabilistic context-free grammars (PCFG), to more rich spatial patterns found in the Abstraction and Reasoning Corpus (ARC), a general AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern completion proficiency can be partially retained even when the sequences are expressed using tokens randomly sampled from the vocabulary. These results suggest that without any additional training, LLMs can serve as general sequence modelers, driven by in-context learning. In this work, we investigate how these zero-shot capabilities may be applied to problems in robotics—from extrapolating sequences of numbers that represent states over time to complete simple motions, to least-to-most prompting of reward-conditioned trajectories that can discover and represent closed-loop policies (e.g., a stabilizing controller for CartPole). While difficult to deploy today for real systems due to latency, context size limitations, and compute costs, the approach of using LLMs to drive low-level control may provide an exciting glimpse into how the patterns among words could be transferred to actions.},
	language = {en},
	urldate = {2025-11-28},
	publisher = {arXiv},
	author = {Mirchandani, Suvir and Xia, Fei and Florence, Pete and Ichter, Brian and Driess, Danny and Arenas, Montserrat Gonzalez and Rao, Kanishka and Sadigh, Dorsa and Zeng, Andy},
	month = oct,
	year = {2023},
	note = {arXiv:2307.04721 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, Computer Science - Computation and Language},
	annote = {Comment: 21 pages, 25 figures. To appear at Conference on Robot Learning (CoRL) 2023},
}

@misc{mirchandani_large_2023-1,
	title = {Large {Language} {Models} as {General} {Pattern} {Machines}},
	url = {http://arxiv.org/abs/2307.04721},
	doi = {10.48550/arXiv.2307.04721},
	abstract = {We observe that pre-trained large language models (LLMs) are capable of autoregressively completing complex token sequences—from arbitrary ones procedurally generated by probabilistic context-free grammars (PCFG), to more rich spatial patterns found in the Abstraction and Reasoning Corpus (ARC), a general AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern completion proficiency can be partially retained even when the sequences are expressed using tokens randomly sampled from the vocabulary. These results suggest that without any additional training, LLMs can serve as general sequence modelers, driven by in-context learning. In this work, we investigate how these zero-shot capabilities may be applied to problems in robotics—from extrapolating sequences of numbers that represent states over time to complete simple motions, to least-to-most prompting of reward-conditioned trajectories that can discover and represent closed-loop policies (e.g., a stabilizing controller for CartPole). While difficult to deploy today for real systems due to latency, context size limitations, and compute costs, the approach of using LLMs to drive low-level control may provide an exciting glimpse into how the patterns among words could be transferred to actions.},
	language = {en},
	urldate = {2025-11-28},
	publisher = {arXiv},
	author = {Mirchandani, Suvir and Xia, Fei and Florence, Pete and Ichter, Brian and Driess, Danny and Arenas, Montserrat Gonzalez and Rao, Kanishka and Sadigh, Dorsa and Zeng, Andy},
	month = oct,
	year = {2023},
	note = {arXiv:2307.04721 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, Computer Science - Computation and Language},
	annote = {Comment: 21 pages, 25 figures. To appear at Conference on Robot Learning (CoRL) 2023},
}

@misc{tan_lxmert_2019,
	title = {{LXMERT}: {Learning} {Cross}-{Modality} {Encoder} {Representations} from {Transformers}},
	shorttitle = {{LXMERT}},
	url = {http://arxiv.org/abs/1908.07490},
	doi = {10.48550/arXiv.1908.07490},
	abstract = {Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22\% absolute (54\% to 76\%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results; and also present several attention visualizations for the different encoders. Code and pre-trained models publicly available at: https://github.com/airsplay/lxmert},
	language = {en},
	urldate = {2025-11-28},
	publisher = {arXiv},
	author = {Tan, Hao and Bansal, Mohit},
	month = dec,
	year = {2019},
	note = {arXiv:1908.07490 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: EMNLP 2019 (14 pages; with new attention visualizations)},
}

@article{lin_grounded_2023,
	title = {On {Grounded} {Planning} for {Embodied} {Tasks} with {Language} {Models}},
	volume = {37},
	issn = {2374-3468, 2159-5399},
	url = {http://arxiv.org/abs/2209.00465},
	doi = {10.1609/aaai.v37i11.26549},
	abstract = {Language models (LMs) have demonstrated their capability in possessing commonsense knowledge of the physical world, a crucial aspect of performing tasks in everyday life. However, it remains unclear **whether LMs have the capacity to generate grounded, executable plans for embodied tasks.** This is a challenging task as LMs lack the ability to perceive the environment through vision and feedback from the physical environment. In this paper, we address this important research question and present the first investigation into the topic. Our novel problem formulation, named **G-PlanET**, inputs a high-level goal and a data table about objects in a specific environment, and then outputs a step-by-step actionable plan for a robotic agent to follow. To facilitate the study, we establish an **evaluation protocol** and design a dedicated metric to assess the quality of the plans. Our experiments demonstrate that the use of tables for encoding the environment and an iterative decoding strategy can significantly enhance the LMs' ability in grounded planning. Our analysis also reveals interesting and non-trivial findings.},
	language = {en},
	number = {11},
	urldate = {2025-11-28},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Lin, Bill Yuchen and Huang, Chengsong and Liu, Qian and Gu, Wenda and Sommerer, Sam and Ren, Xiang},
	month = jun,
	year = {2023},
	note = {arXiv:2209.00465 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, Computer Science - Computation and Language, Computer Science - Machine Learning},
	pages = {13192--13200},
	annote = {Comment: Accepted to AAAI 2023 Project website: https://yuchenlin.xyz/g-planet/},
}

@article{ahn_as_nodate,
	title = {Do {As} {I} {Can}, {Not} {As} {I} {Say}: {Grounding} {Language} in {Robotic} {Affordances}},
	abstract = {Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a signiﬁcant weakness of language models is that they lack real-world experience, which makes it difﬁcult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model’s “hands and eyes,” while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project’s website, video, and open source can be found at say-can.github.io.},
	language = {en},
	author = {Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Fu, Chuyuan and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Ho, Daniel and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jang, Eric and Ruano, Rosario Jauregui and Jeffrey, Kyle and Jesmonth, Sally and Joshi, Nikhil J and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Luu, Linda and Parada, Carolina and Pastor, Peter and Quiambao, Jornell and Rao, Kanishka and Rettinghouse, Jarek and Reyes, Diego and Sermanet, Pierre and Sievers, Nicolas and Tan, Clayton and Toshev, Alexander and Vanhoucke, Vincent and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yan, Mengyuan and Zeng, Andy},
}

@article{brown_language_nodate,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-ofthe-art ﬁne-tuning approaches. Speciﬁcally, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous nonsparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or ﬁne-tuning, with tasks and few-shot demonstrations speciﬁed purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
	language = {en},
	author = {Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
}

@article{radford_learning_nodate,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	abstract = {SOTA computer vision systems are trained to predict a ﬁxed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efﬁcient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study performance on over 30 different computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of ﬁne-grained object classiﬁcation. The model transfers nontrivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset speciﬁc training. For instance, we match the accuracy of the original ResNet50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	language = {en},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
}

@article{yi_neural-symbolic_nodate,
	title = {Neural-{Symbolic} {VQA}: {Disentangling} {Reasoning} from {Vision} and {Language} {Understanding}},
	abstract = {We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our neural-symbolic visual question answering (NS-VQA) system ﬁrst recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three unique advantages. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of 99.8\% on the CLEVR dataset. Second, the model is more data- and memory-efﬁcient: it performs well after learning on a small number of training data; it can also encode an image into a compact representation, requiring less storage than existing methods for ofﬂine question answering. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step.},
	language = {en},
	author = {Yi, Kexin and Wu, Jiajun and Gan, Chuang},
}

@misc{pi_reasoning_2022,
	title = {Reasoning {Like} {Program} {Executors}},
	url = {http://arxiv.org/abs/2201.11473},
	doi = {10.48550/arXiv.2201.11473},
	abstract = {Reasoning over natural language is a longstanding goal for the research community. However, studies have shown that existing language models are inadequate in reasoning. To address the issue, we present POET, a novel reasoning pre-training paradigm. Through pretraining language models with programs and their execution results, POET empowers language models to harvest the reasoning knowledge possessed by program executors via a data-driven approach. POET is conceptually simple and can be instantiated by different kinds of program executors. In this paper, we showcase two simple instances POET-Math and POET-Logic, in addition to a complex instance, POET-SQL. Experimental results on six benchmarks demonstrate that POET can signiﬁcantly boost model performance in natural language reasoning, such as numerical reasoning, logical reasoning, and multi-hop reasoning. POET opens a new gate on reasoningenhancement pre-training, and we hope our analysis would shed light on the future research of reasoning like program executors.},
	language = {en},
	urldate = {2025-11-30},
	publisher = {arXiv},
	author = {Pi, Xinyu and Liu, Qian and Chen, Bei and Ziyadi, Morteza and Lin, Zeqi and Fu, Qiang and Gao, Yan and Lou, Jian-Guang and Chen, Weizhu},
	month = oct,
	year = {2022},
	note = {arXiv:2201.11473 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Symbolic Computation},
	annote = {Comment: To appear in EMNLP 2022 main conference. The first two authors contributed equally},
}

@misc{tan_sim--real_2018,
	title = {Sim-to-{Real}: {Learning} {Agile} {Locomotion} {For} {Quadruped} {Robots}},
	shorttitle = {Sim-to-{Real}},
	url = {http://arxiv.org/abs/1804.10332},
	doi = {10.48550/arXiv.1804.10332},
	abstract = {Designing agile locomotion for quadruped robots often requires extensive expertise and tedious manual tuning. In this paper, we present a system to automate this process by leveraging deep reinforcement learning techniques. Our system can learn quadruped locomotion from scratch using simple reward signals. In addition, users can provide an open loop reference to guide the learning process when more control over the learned gait is needed. The control policies are learned in a physics simulator and then deployed on real robots. In robotics, policies trained in simulation often do not transfer to the real world. We narrow this reality gap by improving the physics simulator and learning robust policies. We improve the simulation using system identiﬁcation, developing an accurate actuator model and simulating latency. We learn robust controllers by randomizing the physical environments, adding perturbations and designing a compact observation space. We evaluate our system on two agile locomotion gaits: trotting and galloping. After learning in simulation, a quadruped robot can successfully perform both gaits in the real world.},
	language = {en},
	urldate = {2025-11-30},
	publisher = {arXiv},
	author = {Tan, Jie and Zhang, Tingnan and Coumans, Erwin and Iscen, Atil and Bai, Yunfei and Hafner, Danijar and Bohez, Steven and Vanhoucke, Vincent},
	month = may,
	year = {2018},
	note = {arXiv:1804.10332 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	annote = {Comment: Accompanying video: https://www.youtube.com/watch?v=lUZUr7jxoqM},
}
