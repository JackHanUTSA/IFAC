\section{Architecture}\label{architecture}

The \textbf{Dynamic Penetrative Trajectory Adaptation (DPTA)} framework is a hierarchical robotic architecture that integrates large-scale language-based reasoning, physically grounded sensory interpretation, adaptive trajectory learning, and reinforcement learning. The system enables a robot to interpret natural language instructions, perceive through multimodal signals, and adapt its motion in real-world conditions (see Figure~\ref{fig:System}).

\begin{figure*}[!t]
   % also try [h], [tb], [!t], etc.
\centering
\includegraphics[width=\textwidth]{IFAC.002.png}
\caption{System demonstration}
\label{fig:System}
\end{figure*}

\[
\mathcal{A}_{\text{DPTA}} = 
\{ \mathcal{L}_{\text{plan}}, \mathcal{L}_{\text{perc}}, 
\mathcal{L}_{\text{exec}}, \mathcal{L}_{\text{learn}} \}.
\]

\subsection{High-Level Reasoning and Planning Layer}
At the highest level, a Large Language Model (LLM) acts as the cognitive planner. Given a natural-language instruction (e.g., ``open the cabinet''), the LLM performs long-horizon task decomposition and generates a structured sequence of sub-tasks. Each sub-task is mapped to motion primitives from a predefined \emph{Basic Motion Library}, enabling generalized planning without task-specific programming.

\subsection{Penetrative Perception Layer}
Unlike vision-centric pipelines, DPTA leverages \textit{Penetrative AI}, see \cite{xu2024penetrative} to process raw sensor signals (e.g., joint torques, force/torque sensors). Using embedded knowledge and numerical reasoning, the LLM infers latent physical properties such as material stiffness, friction, compliance, or slippage. This enables detection of micro-level physical states that are difficult to capture visually.

\subsection{Dynamic Execution Layer}
When a task exceeds the capabilities of predefined motion 
primitives, DPTA employs \textit{Dynamic Movement Primitives 
(DMP)}, see \cite{wu2023daydreamer} combined with \textit{Human-Robot Collaboration 
(HRC)}, see \cite{andrychowicz2017hindsight}. Demonstrated trajectories are encoded as DMPs and 
stored in a library for future reuse, allowing continual expansion of manipulation skills.

\subsection{Dynamic Execution Layer (Trajectory Adaptation via DMP)}

This layer utilizes \textbf{Dynamic Movement Primitives (DMP)} to encode and reproduce complex behaviors learned from human demonstration. 
DMPs allow the system to generalize movements to new spatial targets while preserving the original trajectory shape.

The transformation system in DMPs can be expressed as a springâ€“damper system driven by a nonlinear forcing function:

\begin{equation}
    \ddot{y} = \alpha \left(\beta (g - y) - \dot{y}\right) + f(x)(g - y)
\end{equation}

where:
\begin{itemize}
    \item $y$ is the system state (e.g., end-effector coordinates or joint angle).
    \item $g$ is the goal state.
    \item $\alpha, \beta$ are positive gain constants.
    \item $f(x)$ is the nonlinear forcing term that shapes the trajectory.
\end{itemize}

The forcing term is defined as a weighted sum of Gaussian basis functions:

\begin{equation}
    f(x) = 
    \frac{\displaystyle \sum_{i=1}^{N} \psi_i w_i x}
         {\displaystyle \sum_{i=1}^{N} \psi_i}
\end{equation}

where:
\begin{itemize}
    \item $\psi_i$ are Gaussian basis functions,
    \item $w_i$ are learnable weights obtained from human demonstration,
    \item $x$ is the canonical phase variable, decaying monotonically over time,
    \item $N$ is the number of basis functions (set to $N=15$ in our implementation).
\end{itemize}

By modifying only the goal state $g$ or start state $y_0$, the DPTA framework can generate new adaptive trajectories while maintaining the learned motion structure.
This enables smooth re-targeting and robust trajectory reshaping without retraining.


\subsection{Robust Learning Layer}
To ensure continuous improvement, DPTA incorporates 
\textit{Hindsight Experience Replay (HER)}, see \cite{andrychowicz2017hindsight}. 
Sparse rewards in manipulation tasks are relabeled, 
allowing failed trajectories to contribute to learning 
alternative goals. This approach improves sample efficiency 
and policy robustness, enabling the system to autonomously 
adapt over time.

% \[
% \text{DPTA} = \mathcal{F}\big(
%     \mathcal{P}_{\text{PenAI}},\;
%     \mathcal{T}_{\text{DMP/HRC}},\;
%     \mathcal{R}_{\text{RRL}}
% \big)
% \]




%\begin{eqnarray}\label{s6}
%\dot{x}=
%\left[
%\begin{array}{l}
%	x_{2}^{p}\\
%	x_{3}\\
%	-a_{3}\left(a_{1}x_{1}+a_{2}x_{2}^{\frac{p+1}{2}}+x_{3}\right)^{\frac{2p}{p+1}}
%\end{array}
%\right]
%=H(t,x).
%\end{eqnarray}

