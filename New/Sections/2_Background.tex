%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background}\label{background}

The complex nature of interactive robot missions, particularly those involving high-level 
language models (LLMs) or sophisticated reinforcement learning (RL), 
presents numerous challenges across planning, learning, perception, and execution.



\subsection{Main challenges}

Limitations in High-Level Planning and Trajectory Execution (LLMs):
When LLMs are used for autonomous manipulation, they encounter issues related to generating and executing physical motions:
Inability to Handle Complex Trajectories(\cite{liu2024enhancing}) (Feasibility Issues): The conventional approach of 
using LLMs to generate code for robot motion falters when dealing with complex trajectories. 
Tasks that require intricate trajectory planning and reasoning over environments, such as 
opening an oven door featuring a horizontal axis design or opening a cabinet with a press-pull 
structure, may be deemed infeasible when relying only on the basic motion library generated by 
the LLM(\cite{liu2024enhancing}).Fragility of Prompt Design: The current design paradigm for using LLMs as controllers is 
fragile, meaning even minor alterations in the prompt can dramatically affect the performance.\cite{wang2024prompt} 
Designing a reliable prompt for robotic tasks is not yet well understood.
Executability Anomalies: Although generally high, the code generated by the LLM can 
 occasionally generate sub-tasks without assigning corresponding motion 
 functions, resulting in non-encodable and non-executable responses(\cite{liu2024enhancing}).

Issues Related to Perception and Grounding:

Successfully linking high-level instructions to the physical world introduces multiple errors:
Environmental Perception Errors and Error Accumulation: Real-world task success rates decrease 
due to error accumulation across sequential sub-tasks(\cite{liu2024enhancing}). 
Errors in environmental perception stem from inaccuracies in object detection models (
   like YOLOv5), 
such as bounding box inaccuracies, leading to slightly variable coordinates for target objects.\cite{liu2024enhancing} 
These discrepancies can cause the errors to exceed the necessary margins for precise manipulation 
(e.g., placing an apple into an oven with minimal clearance)(\cite{liu2024enhancing})
Sensor Data Processing Limitations: LLMs, when used in a "penetrative" way to analyze 
digitized sensor signals (like sequences of ECG digits), exhibit lower 
efficiency in processing extensive sequences of digital data compared to traditional methods.
 The hallucination rates and Mean Absolute Errors (MAEs) for some LLMs escalate with the 
 increase in window size of the input data, suggesting an inherent limitation in processing 
 extensive lengths of digitized sequences, see \cite{xu2024penetrative}.
Susceptibility to Deployment Noise: Policies trained in simulation, even when using modern techniques, 
may not be robust to real-world noise. For instance, a policy trained in simulation for pick-and-place was not 
robust to small errors in box position estimation (e.g., errors with a standard deviation of 1cm) when deployed 
on a physical robot(\cite{andrychowicz2017hindsight}).

Challenges in Low-Level Control:

Controllability and Security Risks: Since LLM responses are probabilistic, there is no guarantee that the swarm will behave as intended. This also introduces new security vulnerabilities, as it needs to be studied if users or even other robots can reprogram robots through prompt injection attacks or 
if a malicious agent could send misleading information (Byzantine robot detection)(\cite{strobel2024llm2swarm}).



\subsection{ Overview: Dynamic Penetrative Trajectory Adaptation (DPTA)}

In current LLM-based manipulation, environmental information is 
primarily derived visually (e.g., YOLOv5 for object position)(\cite{liu2024enhancing}). 
However, fine-grained tasks often depend on non-visual physical 
feedback (e.g., force or torque required to open a tight hinge). 
In DPTA, the Penetrative AI(\cite{xu2024penetrative}) paradigm is employed to process digitized sensor 
signals from the robot's end effector 
(e.g., force/torque sensors, joint current feedback)


LLM's task is not to determine a broad state (like "indoors/outdoors"), 
but to execute a real-time, micro-level physical classification of the 
object/environment state during the initial phase of interaction 
(e.g., the first 100 milliseconds of grasping a handle or pushing a button)\cite{bhat2024grounding}.

Prompt Design: Our prompt utilizes the procedural guidance and fuzzy logic methods demonstrated in 
the heart rate detection task.(\cite{xu2024penetrative}) It contains a short sequence of 
raw numerical feedback (avoiding the token limit constraint associated with 
long digitized sequences) and instructs the LLM to classify the physical anomaly based on 
relative changes in the sequence.

Example Output: Based on the input torque sequence, the LLM reasoning(\cite{liu2024enhancing}) determines the precise 
physical condition, such as "Horizontal-Axis Oven Door, 
stiff hinge" or "Press-Pull Cabinet, high friction."

Dynamic Controller Role: The LLM functions as a dynamic feedback controller, selecting between:





\subsection{Literature Review}

The proposed Dynamic Penetrative Trajectory Adaptation (DPTA) framework differentiates itself from existing research in LLM-controlled robotics by addressing limitations in physical grounding, low-level trajectory execution, and robust policy learning.


High-Level Planning with Imperfect Grounding
Existing works leverage large language models (LLMs) as zero-shot planners to decompose high-level instructions (e.g., "make breakfast") 
into a sequence of actionable steps [\cite{huang2022language}]. Methods like those involving Semantic Translation improve executability 
by mapping LLM-generated phrases to the most semantically similar admissible action in a predefined set. 
SayCan grounds LLM output by weighting actions based on learned skill affordances (value functions), 
ensuring the proposed step is feasible in the current state [\cite{brohan2023can}].

Limitation: Reliance on Narrow APIs and Lack of Deep Physical Context

Despite these efforts, LLM-generated plans are still frequently not executable in interactive environments (\cite{huang2022language}) and 
struggle with mid-level grounding, often missing necessary common-sense actions [\cite{huang2022language}].    
Critically, most systems rely on predetermined vision APIs (like object detectors) to describe the scene [\cite{vemprala2024chatgpt}]. 
This focus on visual data ignores crucial non-visual physical feedback (e.g., force or torque) needed for fine-grained tasks [\cite{huang2023inner}], 
leaving the LLM policies restricted by what the perception APIs can describe [\cite{brown2020language}].

Code Generation and Trajectory Management

Recent research utilizes LLMs to directly generate policy code (Code as Policies), allowing the model to compose 
perception-to-control logic and reference external libraries (like NumPy) for complex behaviors such as spatial-geometric reasoning [\cite{liang2023code}]. 
Furthermore, LLMs have been explored as low-level feedback policies for dynamic systems like robot walking by outputting target joint positions 
directly from historical input-output sequences [\cite{liang2023code}].


Closed-Loop Adaptation and Policy Learning

Systems such as Inner Monologue and Socratic Models integrate closed-loop feedback by feeding natural language 
observations (e.g., success detection or scene description updates) back into the LLM prompt, enabling the LLM planner to reason over outcomes and 
dynamically re-plan actions [huang2023inner]. In the domain of reinforcement learning (RL), solving robotic tasks often faces the critical challenge of 
sparse and binary rewards [\cite{andrychowicz2017hindsight}].

Feedback Limitations and Learning Inefficiency

The effectiveness of closed-loop reasoning is bottlenecked by the capabilities of the low-level control policies and the fidelity of the 
language description provided by the perception system [\cite{gal2016dropout}, \cite{ebert2018internet}]. Moreover, traditional RL systems often fail in large state spaces when faced with 
sparse rewards, often necessitating tedious and domain-specific reward function engineering [\cite{andrychowicz2017hindsight}].





